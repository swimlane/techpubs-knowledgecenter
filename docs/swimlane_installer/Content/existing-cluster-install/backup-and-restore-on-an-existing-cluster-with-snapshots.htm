<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" style="mc-template-page: url('../Resources/MasterPages/OtherTopics.flmsp');">
    <head>
        <link href="../Resources/TableStyles/Alternate-Row-Color.css" rel="stylesheet" MadCap:stylesheetType="table" /><title></title>
        <link href="../Resources/Stylesheets/MainStyles.css" rel="stylesheet" type="text/css" />
    </head>
    <body>
        <h1 id="backup-and-restore-on-an-existing-cluster-with-snapshots">Backup and Restore on an Existing Cluster with Snapshots</h1>
        <p>The Swimlane Platform Installer (SPI) uses Velero and Restic as the underlying technology for snapshots and restores.</p>
        <p class="important">Important! Note the following bulleted items that list limitations with snapshots: <br /><br />Taking a snapshot requires enough free disk space for a compressed archive of the Swimlane database to be saved in ephemeral storage before it is uploaded to the snapshot destination. Free disk space on the cluster at /var/lib/kubelet should be greater than or equal to the size of the uncompressed database to ensure there is no disk pressure during the snapshot process.
<br /><br />Instance snapshots are considered complete and are usable in disaster recovery scenarios. Application snapshots are the legacy snapshot method and are not usable for disaster recovery. Instance snapshots can be used in place of the legacy application snapshots.
<br /><br />Snapshots cannot be restored to a different namespace than when the snapshot was taken. <br /><br />Snapshots cannot be restored to different installation methods. For example, online cluster snapshots cannot be restored to offline clusters.
<br /><br />AWS S3 buckets with a bucket policy that requires the server-side encryption header are not supported. If you need to require server-side encryption for objects, Swimlane recommends that you enable default encryption on the bucket itself instead.<br /><![CDATA[
]]><br />Cleanup and removal of snapshots can only be done through the SPI admin console snapshots tab. Removing data from the snapshot storage itself will result in data corruption and loss of snapshots.</p>
        <h2 id="install-velero"><a name="Install"></a>Install Velero</h2>
        <p>See Velero's <a href="https://velero.io/docs/v1.9/customize-installation/#customize-resource-requests-and-limits" target="_blank_">Customize Resource Requests and Limits</a> if you need to set the resource requests and limits for the Velero and Restic pods.</p>
        <h3 id="install-velero-online">Install Velero (Online)</h3>
        <p>Velero install instructions if your cluster has access to the internet:</p>
        <ol>
            <li>
                <p>Download Velero 1.9.0 for the OS version from which you run kubectl commands: <br /></p>
                <MadCap:codeSnippet>
                    <MadCap:codeSnippetCopyButton />
                    <MadCap:codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">https://github.com/vmware-tanzu/velero/releases/tag/v1.9.0</MadCap:codeSnippetBody>
                </MadCap:codeSnippet>
            </li>
            <li>
                <p>Untar the file and then change directory into the uncompressed directory:<br /></p>
                <MadCap:codeSnippet>
                    <MadCap:codeSnippetCopyButton />
                    <MadCap:codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">tar zxf &lt;FILENAME&gt;.tar.gz
cd &lt;DIRECTORY&gt;</MadCap:codeSnippetBody>
                </MadCap:codeSnippet>
            </li>
        </ol>
        <ol start="3">
            <li>Install Velero with the necessary plugins and placeholder information:</li>
            <p>Create an empty credentials file to pass to the install command with touch placeholder-credentials.</p>
            <p>The Velero binary must be executable by your user. Install Velero into the velero namespace with the following command:</p>
            <MadCap:codeSnippet>
                <codeSnippetCopyButton />
                <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">  ./velero install \
    --use-restic \
    --use-volume-snapshots=false \
    --plugins velero/velero-plugin-for-aws:v1.5.0,velero/velero-plugin-for-gcp:v1.5.0,velero/velero-plugin-for-microsoft-azure:v1.5.0 \
    --provider aws \
    --bucket placeholder \
    --prefix /placeholder/ \
    --secret-file placeholder-credentials \
    --backup-location-config region=us-east-1</codeSnippetBody>
            </MadCap:codeSnippet>
            <p>The bucket, prefix, credentials, and bucket region settings are set as placeholders in this command just to get Velero installed. You will update the snapshot storage destination later through the SPI UI.</p>
            <p>If the KUBECONFIG environment variable is not set you will need to add --kubeconfig /path/to/kube/config to the Velero install command above so that it can authenticate to your cluster.</p>
            <p>See Velero's <a href="https://velero.io/docs/v1.9/customize-installation/#customize-resource-requests-and-limits" target="_blank_">Customize Resource Requests and Limits</a> if you need to set the resource requests and limits for the Velero and Restic pods.</p>
        </ol>
        <h3 id="install-velero-offlineairgapped">Install Velero (Offline/Airgapped)</h3>
        <p>Use these Velero install instructions if your cluster has no access to the internet:</p>
        <ol>
            <li>
                <p>Download Velero 1.9.0 to the jumpbox for the jumpbox's OS version: <br /></p>
            </li>
            <MadCap:codeSnippet>
                <MadCap:codeSnippetCopyButton />
                <MadCap:codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">https://github.com/vmware-tanzu/velero/releases/tag/v1.9.0</MadCap:codeSnippetBody>
            </MadCap:codeSnippet>
            <li>
                <p>Untar the file and then change directory into the uncompressed directory:</p>
                <MadCap:codeSnippet>
                    <MadCap:codeSnippetCopyButton />
                    <MadCap:codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">tar zxf &lt;FILENAME&gt;.tar.gz
cd &lt;DIRECTORY&gt;
</MadCap:codeSnippetBody>
                </MadCap:codeSnippet>
            </li>
        </ol>
        <ol start="2">
            <li>Download the Velero images and push them to your private registry from your jumpbox:</li>
            <MadCap:codeSnippet>
                <codeSnippetCopyButton />
                <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">docker pull velero/velero:v1.9.0
  docker tag velero/velero:v1.9.0 &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero:v1.9.0
  docker push &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero:v1.9.0

  docker pull velero/velero-restic-restore-helper:v1.9.0
  docker tag velero/velero-restic-restore-helper:v1.9.0 &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-restic-restore-helper:v1.9.0
  docker push &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-restic-restore-helper:v1.9.0

  docker pull velero/velero-plugin-for-aws:v1.5.0
  docker tag velero/velero-plugin-for-aws:v1.5.0 &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-aws:v1.5.0
  docker push &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-aws:v1.5.0

  docker pull velero/velero-plugin-for-gcp:v1.5.0
  docker tag velero/velero-plugin-for-gcp:v1.5.0 &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-gcp:v1.5.0
  docker push &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-gcp:v1.5.0

  docker pull velero/velero-plugin-for-microsoft-azure:v1.5.0
  docker tag velero/velero-plugin-for-microsoft-azure:v1.5.0 &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-microsoft-azure:v1.5.0
  docker push &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-microsoft-azure:v1.5.0</codeSnippetBody>
            </MadCap:codeSnippet>
        </ol>
        <ol start="3">
            <li>Create the Velero restore helper, ConfigMap, to ensure that the Velero helper init containers pull from the private registry. Create the file velero-restore-helper-configmap.yaml with the following contents:</li>
            <MadCap:codeSnippet>
                <codeSnippetCopyButton />
                <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: restic-restore-action-config
    namespace: velero
    labels:
      velero.io/plugin-config: ""
      velero.io/restic: RestoreItemAction
  data:
    image: &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-restic-restore-helper</codeSnippetBody>
            </MadCap:codeSnippet>
            <p>Create the config map by running kubectl apply -f velero-restore-helper-configmap.yaml.</p>
            <p>See Velero's <a href="https://velero.io/docs/v1.9/restic/#customize-restore-helper-container" target="_blank_">Customize Restore Helper Container</a> if you need to set the resource requests and limits for the Velero restore helper container.</p>
        </ol>
        <ol start="4">
            <li>Install Velero with the necessary plugins and placeholder information:</li>
            <p>Create an empty credentials file to pass to the install command with touch placeholder-credentials.</p>
            <p>The Velero binary must be executable by your user. Install Velero into the velero namespace with the following command:</p>
            <MadCap:codeSnippet>
                <codeSnippetCopyButton />
                <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">./velero install \
    --image &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero:v1.9.0 \
    --use-restic \
    --use-volume-snapshots=false \
    --plugins &lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-aws:v1.5.0,&lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-gcp:v1.5.0,&lt;my-registry&gt;/&lt;my-namespace&gt;/velero/velero-plugin-for-microsoft-azure:v1.5.0 \
    --provider aws \
    --bucket placeholder \
    --prefix /placeholder/ \
    --secret-file placeholder-credentials \
    --backup-location-config region=us-east-1</codeSnippetBody>
            </MadCap:codeSnippet>
            <p>The bucket, prefix, credentials, and bucket region settings are set as placeholders in this command just to get Velero installed. You will update the snapshot storage destination later through the SPI UI.</p>
            <p>The Velero install uses the KUBECONFIG environment variable to authenticate to the cluster. If that environment variable is not set you must add the argument --kubeconfig /path/to/kube/config to the install command above.</p>
        </ol>
        <h3 id="install-velero-openshift">Install Velero (OpenShift)</h3>
        <ol>
            <li>
                <p>Install Velero using the Install Velero (Online) instructions above.</p>
            </li>
            <li>
                <p>Set the required OpenShift policy for the Velero service account to let the Velero pods run:<br /></p>
            </li>
            <MadCap:codeSnippet>
                <MadCap:codeSnippetCopyButton />
                <MadCap:codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">oc adm policy add-scc-to-user privileged -z velero -n velero</MadCap:codeSnippetBody>
            </MadCap:codeSnippet>
        </ol>
        <ol start="2">
            <li>Update the Velero Restic daemonset to run the pods with the required security context:</li>
            <MadCap:codeSnippet>
                <codeSnippetCopyButton />
                <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">oc patch ds/restic \
        --namespace velero \
        --type json \
        -p '[{"op":"add","path":"/spec/template/spec/containers/0/securityContext","value": { "privileged": true}}]'</codeSnippetBody>
            </MadCap:codeSnippet>
        </ol>
        <h2 id="configure-snapshot-storage">Configure Snapshot Storage</h2>
        <p>From the top-level Swimlane Platform Installer UI, click the Snapshots section and then the Settings &amp; Schedule link on the right side.</p>
        <p>
            <img src="../Resources/Images/snapshot_settings.png" alt="" />
        </p>
        <p class="note"><b>Note:</b> When the snapshot settings are updated the Velero and Restic pods are restarted to apply the new settings. This can take several seconds and can temporarily make the snapshot pages show that Velero isn't running. Once the pods are restarted, the snapshot pages correctly display.</p>
        <h2 id="storing-snapshots-on-amazon-s3">Storing Snapshots on Amazon S3</h2>
        <p>
            <img src="../Resources/Images/snapshot_settings_s3.png" width="50%" alt="" />
        </p>
        <h3 id="requirements">Requirements</h3>
        <p>Storing snapshots on Amazon S3 requires:</p>
        <ul>
            <li>
                <p>An IAM user or IAM role to authenticate.</p>
            </li>
            <li>
                <p>The bucket cannot have a bucket policy that requires the server-side encryption header. The recommended method to require server side encryption for objects is to enable default encryption on the bucket itself instead.</p>
            </li>
            <li>
                <p>The following sample policy can be used after replacing ${BUCKET} with the AWS ARN of your bucket:</p>
                <MadCap:codeSnippet>
                    <codeSnippetCopyButton />
                    <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">{
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": [
                  "s3:GetObject",
                  "s3:DeleteObject",
                  "s3:PutObject",
                  "s3:AbortMultipartUpload",
                  "s3:ListMultipartUploadParts"
              ],
              "Resource": [
                  "arn:aws:s3:::${BUCKET}/*"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:ListBucket"
              ],
              "Resource": [
                  "arn:aws:s3:::${BUCKET}"
              ]
          }
      ]
  }
</codeSnippetBody>
                </MadCap:codeSnippet>
            </li>
        </ul>
        <h3 id="instructions">Instructions</h3>
        <ol>
            <li>
                <p>Change the Destination drop down to Amazon S3.</p>
            </li>
            <li>
                <p>Set Bucket to the name of the Amazon S3 bucket to store snapshots in.</p>
            </li>
            <li>
                <p>Set Region to the name of the AWS region that the S3 bucket is in.</p>
            </li>
            <li>
                <p>Set Path to the path in the S3 bucket that the snapshots should be stored under.</p>
            </li>
            <li>
                <p>If your cluster nodes are AWS EC2 instances and you want the AWS permissions to access the S3 bucket managed by an IAM instance role, check the Use IAM Instance Role checkbox and leave the Access Key ID and Access Key Secret fields blank.</p>
            </li>
            <li>
                <p>If you need to use IAM credentials to access the S3 bucket then set Access Key ID and Access Key Secret to the IAM user's API credentials.</p>
            </li>
        </ol>
        <h2 id="storing-snapshots-on-azure-blob-storage">Storing Snapshots on Azure Blob Storage</h2>
        <p>
            <img src="../Resources/Images/snapshot_settings_azure.png" width="50%" alt="" />
        </p>
        <h3 id="requirements-1">Requirements</h3>
        <p>Storing snapshots on Azure Blog Storage requires:</p>
        <ul>
            <li>An Azure service principal and client secret to authenticate.</li>
            <li>The storage account and service principal must be in the same subscription, tenant, and resource group.</li>
            <li>Required service principal permissions:
<ul><li>The service principal must have the Storage Account Key Operator Service Role on the storage account.</li><li>The service principal must have the Storage Blob Data Contributor role on the storage container.</li></ul></li>
        </ul>
        <h3 id="instructions-1">Instructions</h3>
        <ol>
            <li>
                <p>Change the Destination drop down to Azure Blob Storage.</p>
            </li>
            <li>
                <p>Configure your Azure settings:</p>
            </li>
            <table style="mc-table-style: url('../Resources/TableStyles/Alternate-Row-Color.css');" class="TableStyle-Alternate-Row-Color" cellspacing="21">
                <thead>
                    <tr class="TableStyle-Alternate-Row-Color-Head-Header1">
                        <th class="TableStyle-Alternate-Row-Color-HeadE-Column1-Header1">Field</th>
                        <th class="TableStyle-Alternate-Row-Color-HeadD-Column1-Header1">Details</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body1">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body1">Bucket</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body1">Set to the name of the Azure storage container where you will store snapshots.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body2">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body2">Path</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body2">Set to the path in the Azure storage container where the snapshots should be stored.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body1">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body1">Subscription ID</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body1">Set to the Azure subscription ID where your resources are.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body2">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body2">Tenant ID</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body2">Set to the Azure tenant ID where your resources are.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body1">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body1">Client ID</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body1">Set to the client ID of the Azure application that the service principal is a part of.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body2">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body2">Client Secret</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body2">Set to the value of the client secret generated under the Azure service principal.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body1">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body1">Cloud Name</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body1">Set to the Azure cloud where your resources are.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body2">
                        <td class="TableStyle-Alternate-Row-Color-BodyE-Column1-Body2">Resource Group Name</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyD-Column1-Body2">Set to the Azure resource group name where your resources are.</td>
                    </tr>
                    <tr class="TableStyle-Alternate-Row-Color-Body-Body1">
                        <td class="TableStyle-Alternate-Row-Color-BodyB-Column1-Body1">Storage Account ID</td>
                        <td class="TableStyle-Alternate-Row-Color-BodyA-Column1-Body1">Set to the name of the Azure storage account where the storage container is.</td>
                    </tr>
                </tbody>
            </table>
        </ol>
        <h2 id="storing-snapshots-on-google-cloud-storage">Storing Snapshots on Google Cloud Storage</h2>
        <p>
            <img src="../Resources/Images/snapshot_settings_google.png" width="50%" alt="" />
        </p>
        <h3 id="requirements-2">Requirements</h3>
        <p>Storing snapshots on Google Cloud Storage requires:</p>
        <ul>
            <li>Requires a Google Cloud service account to authenticate.</li>
            <li>The service account should have the storage.objectAdmin role on the bucket.</li>
        </ul>
        <h3 id="instructions-2">Instructions</h3>
        <ol>
            <li>
                <p>Change the Destination drop down to Google Cloud Storage.</p>
            </li>
            <li>
                <p>Set Bucket to the name of the Google storage bucket to store snapshots in.</p>
            </li>
            <li>
                <p>Set Path to the path in the bucket that the snapshots should be stored under.</p>
            </li>
            <li>
                <p>If your cluster nodes are Google Cloud VMs and you want the AWS permissions to access the Google Cloud Storage bucket managed by an IAM instance role, check the Use IAM Instance Role checkbox and leave the JSON File field blank.</p>
            </li>
            <li>
                <p>If you need to use IAM credentials to access the Google Cloud Storage bucket then set JSON File to the JSON key for the service account.</p>
            </li>
        </ol>
        <h2 id="storing-snapshots-on-a-host-path">Storing Snapshots on a Host Path</h2>
        <p>
            <img src="../Resources/Images/snapshot_settings_hostpath.png" width="50%" alt="" />
        </p>
        <h3 id="requirements-3">Requirements</h3>
        <p>Storing snapshots on a Host Path requires:</p>
        <ul>
            <li>The host path storage destination should not be used for production environments. They provide a security risk and the snapshots are not stored externally. Restoration will not be possible in the event of a total cluster loss.</li>
            <li>The host path must be a dedicated directory. Do not use a partition used by a service like Docker or Kubernetes for ephemeral storage.</li>
            <li>The host path directory specified must exist on every node that the SPI pods can be scheduled on to ensure snapshots work even if pod scheduling changes.</li>
            <li>The host path directory must be read-writable by the user:group 1001:1001</li>
            <li>Host path cannot be used if your cluster requires pods to have resources, service account, affinity, node selectors, or tolerations defined.
<ul><li>This option creates a Minio deployment in the namespace that Swimlane is installed under to handle passing the snapshot data to the host path. Swimlane does not support changing any of those settings for this deployment.</li></ul></li>
        </ul>
        <h3 id="instructions-3">Instructions</h3>
        <ol>
            <li>Change the Destination drop down to Host Path.</li>
            <li>Set Host Path to the directory on the cluster nodes that the snapshots should be stored under.</li>
        </ol>
        <h2 id="storing-snapshots-on-nfs">Storing Snapshots on NFS</h2>
        <p>
            <img src="../Resources/Images/snapshot_settings_nfs.png" width="50%" alt="" />
        </p>
        <h3 id="requirements-4">Requirements</h3>
        <p>Storing snapshots on NFS requires:</p>
        <ul>
            <li>Supports NFSv3 and NFSv4.</li>
            <li>Host/IP authentication must be used as username and password authentication is not supported.</li>
            <li>The NFS server must be configured to allow access from all the nodes in the cluster.</li>
            <li>The NFS directory must be owned by the user:group 1001:1001.</li>
            <li>The target directory needs to be read-writable by the user:group 1001:1001</li>
            <li>All the nodes in the cluster must have the necessary NFS client packages installed to be able to communicate with the NFS server. For example, the nfs-common package is a common package used on Ubuntu.</li>
            <li>Any firewalls must allow traffic between the NFS server and clients</li>
            <li>NFS cannot be used if your cluster requires pods to have resources, service account, affinity, node selectors, or tolerations defined.
<ul><li>This option creates a Minio deployment in the namespace that Swimlane is installed under to handle passing the snapshot data to the host path and it is not currently supported to change any of those settings for this deployment.</li></ul></li>
        </ul>
        <h3 id="instructions-4">Instructions</h3>
        <ol>
            <li>Change the Destination drop down to Network File System (NFS).</li>
            <li>Set Server to the hostname or IP of the NFS server.</li>
            <li>Set Path to the path on the NFS server that the snapshots should be stored under.</li>
        </ol>
        <h2 id="storing-snapshots-on-other-s3-compatible-provider">Storing Snapshots on Other S3-compatible Provider</h2>
        <p>
            <img src="../Resources/Images/snapshot_settings_s3-compatible.png" width="50%" alt="" />
        </p>
        <h3 id="requirements-5">Requirements</h3>
        <p>Storing snapshots on an S3-Compatible Provider requires:</p>
        <ul>
            <li>An S3-compatible provider like <a href="https://min.io/" target="_blank_">min.io</a>.</li>
            <li>The S3-compatible provider should be installed separately from the cluster nodes that Swimlane is installed on to ensure that snapshots are stored externally from the cluster so they can be retrieved in the event of a total cluster loss.</li>
        </ul>
        <h3 id="instructions-5">Instructions</h3>
        <ol>
            <li>
                <p>Change the Destination drop down to Other S3-Compatible Storage.</p>
            </li>
            <li>
                <p>Set Bucket to the name of the S3-compatible bucket to store snapshots in.</p>
            </li>
            <li>
                <p>Set Path to the path in the S3-compatible that the snapshots should be stored under.</p>
            </li>
            <li>
                <p>Set Access Key ID and Access Key Secret to the credentials required to access the storage provider.</p>
            </li>
            <li>
                <p>Set Endpoint to the required value for your storage provider.</p>
            </li>
            <p class="important">Important! If the snapshot endpoint is using HTTPS, it must present a certificate signed by a commonly-trusted public certificate authority. Self-signed and privately signed certificates are not supported at this time.</p>
        </ol>
        <ol start="6">
            <li>Set Region to the required value for your storage provider.</li>
        </ol>
        <h2 id="restore-from-a-partial-application-snapshot">Restore from a Partial (Application) Snapshot</h2>
        <ol>
            <li>
                <p>On the Snapshots page, you can review a list of all of your application snapshots under the Partial Snapshots (Application) menu. Click the circular icon to restore a certain snapshot to your Swimlane instance.</p>
                <p>
                    <img src="../Resources/Images/restore_from_application_snapshot.png" alt="" />
                </p>
            </li>
            <li>
                <p>If you want to restore to the version of the snapshot, click Restore from snapshot. You are then prompted to enter the slug of the snapshot (confirming the slug name). Enter swimlane-platform.</p>
                <p class="important">Important! Restoring to the version you've selected will remove any data since the snapshot was made. In addition, during restoration, your Swimlane instance will not be available and you will not be able to use the Swimlane Installer UI until the restore completes.</p>
            </li>
            <li>
                <p>Return to the main UI. Once your Application Status displays Ready, then you know that both the UI and your Swimlane instance are back up and available again.</p>
                <p>
                    <img src="../Resources/Images/swimlane_ready.png" alt="" />
                </p>
            </li>
        </ol>
        <h2 id="restore-from-a-full-instance-snapshot-in-a-non-dr-scenario">Restore from a Full (Instance) Snapshot in a non-DR scenario</h2>
        <p>Instance snapshots can act as both instance-level snapshots and as application-level snapshots. This section covers restoring the Swimlane application with an instance snapshot.</p>
        <ol>
            <li>
                <p>On the Snapshots page, you can review a list of all of your instance snapshots under the Full Snapshots (Instance) menu. Click the circular icon and select Partial Restore to restore a certain snapshot to your Swimlane instance.</p>
                <p>
                    <img src="../Resources/Images/restore_from_instance_snapshot.png" alt="" />
                </p>
            </li>
            <li>
                <p>You are then prompted to enter the slug of the snapshot (confirming the slug name). Enter swimlane-platform.</p>
                <p class="important"><b>Important!</b> Restoring to the version you've selected removes any data since the snapshot was made. In addition, during restoration, your Swimlane instance is not available and you will not be able to use the SPI until the restore completes.</p>
            </li>
            <li>
                <p>Return to the main UI. Once your Application Status displays Ready, then you know that both the SPI UI and your Swimlane instance are back up and available again.</p>
                <p>
                    <img src="../Resources/Images/swimlane_ready.png" alt="" />
                </p>
            </li>
        </ol>
        <h2 id="restore-from-a-full-instance-snapshot-in-a-dr-scenario">Restore from a Full (Instance) Snapshot in a DR scenario</h2>
        <p>Before continuing, you must ensure that your target cluster is ready for restoration. This includes having velero installed as documented <a href="#Install">here</a>. This procedure also requires the kots kubectl plugin to be installed.</p>
        <p>AWS S3:</p>
        <MadCap:codeSnippet>
            <codeSnippetCopyButton />
            <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">$ kubectl kots velero configure-aws-s3 access-key \
  --namespace &lt;namespace&gt; \
  --access-key-id &lt;s3-secret-access-key-id&gt; \
  --secret-access-key &lt;s3-secret-access-key&gt; \
  --region &lt;s3-bucket-region&gt; \
  --bucket &lt;s3-bucket&gt;
  </codeSnippetBody>
        </MadCap:codeSnippet>
        <p>Other S3:</p>
        <MadCap:codeSnippet>
            <codeSnippetCopyButton />
            <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">$ kubectl kots velero configure-other-s3 \
  --namespace &lt;namespace&gt; \
  --access-key-id &lt;s3-secret-access-key-id&gt; \
  --secret-access-key &lt;s3-secret-access-key&gt; \
  --endpoint &lt;s3-bucket-endpoint&gt; \
  --region &lt;s3-bucket-region&gt; \
  --bucket &lt;s3-bucket&gt;</codeSnippetBody>
        </MadCap:codeSnippet>
        <p>NFS:</p>
        <MadCap:codeSnippet>
            <codeSnippetCopyButton />
            <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">$ kubectl kots velero configure-nfs \
  --namespace &lt;namespace&gt; \
  --nfs-server &lt;nfs-server-fqdn&gt; \
  --nfs-path &lt;export-nfs-path&gt;</codeSnippetBody>
        </MadCap:codeSnippet>
        <p>Hostpath:</p>
        <p>Ensure that your snapshot volume mount is mounted and accessible on each individual node, then:</p>
        <MadCap:codeSnippet>
            <codeSnippetCopyButton />
            <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">$ kubectl kots velero configure-hostpath \
  --namespace &lt;namespace&gt; \
  --hostpath &lt;/path/to/hostpath&gt;</codeSnippetBody>
        </MadCap:codeSnippet>
        <p class="note"><b>Note:</b> If you are in an airgapped installation you also must provide the following arguments: <br /></p>
        <MadCap:codeSnippet>
            <MadCap:codeSnippetCopyButton />
            <MadCap:codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">--kotsadm-namespace, --kotsadm-registry, --registry-password, --registry-username</MadCap:codeSnippetBody>
        </MadCap:codeSnippet>
        <ol start="2">
            <li>A process takes place after configuring the snapshot storage location that discovers which snapshots are available for restore. After a few minutes, you can run the following to show the backups that are available:</li>
        </ol>
        <MadCap:codeSnippet>
            <codeSnippetCopyButton />
            <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">$ kubectl kots backup ls --namespace &lt;namespace&gt;
NAME              STATUS       ERRORS    WARNINGS    STARTED                          COMPLETED                        EXPIRES
instance-nc8rj    Completed    0         0           2021-04-13 15:48:17 +0000 UTC    2021-04-13 15:49:00 +0000 UTC    29d</codeSnippetBody>
        </MadCap:codeSnippet>
        <ol start="3">
            <li>Select the backup you want to restore from the list and restore it via this command:</li>
        </ol>
        <MadCap:codeSnippet>
            <codeSnippetCopyButton />
            <codeSnippetBody MadCap:useLineNumbers="False" MadCap:lineNumberStart="1" MadCap:continue="False" xml:space="preserve">$ kubectl kots restore --from-backup instance-nc8rj
• Deleting Admin Console ✓
• Restoring Admin Console ✓
• Restoring Applications ✓
• Restore completed successfully.</codeSnippetBody>
        </MadCap:codeSnippet>
    </body>
</html>